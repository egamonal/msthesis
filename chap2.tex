\chapter{Related Work}
This chapter is a short introduction to the main knowledge areas involved in the development of this project.
It describes what has been done before in the field and the theory the project is based upon.
It includes the motivation and the steps to reengineer an application, the evolution of web technology from simple documents to \acp{RIA} like the one of this thesis, the \ac{TDD} method and the relationship with \ac{DbC} and finally, FIXME \ac{XML} as an intermediate representation of the model to interoperate between Basestation and Flango \cm in the robot.

\section{Reengineering}
\label{sec:reengineering}
This thesis takes over the work of the past two years at \company in content management.
The initial situation is a system made with \flash and Django called Flango.
The \se and the robot's \cm are \flash applications.
Whereas the first one runs in a modern browser, on desktop computers, the robot's \cm runs in the robot.
The robot is a Linux system and the browser is a QtWebKit widget embedded in the Qt application that governs the behaviour.
The \flash plug-in will not be updated and this part of the system has to be rewritten in a different technology.

Pressman explains the motivation to reengineer a software in \cite{Pressman:2007}:
\begin{quote} 
Consider any technology product that has served you well. 
You use it regularly, but it's getting old. 
It breaks too often, takes longer to repair than you'd like, and no longer represents the newest technology.
What to do? If the product is hardware, you'll likely throw it away and buy a newer model.
But if it's custom-built software, that option may not be available. 
You'll need to rebuild it. 
You'll create a product with added functionality, better performance and reliability, and improved maintainability.
That's what we call reengineering. 
\end{quote}

Unlike other products, software does not degrade with time due to external factors such as the inclemencies of the weather, power outages or intense use.
Software packages are adapted, corrected, extended and improved constantly.
That can lead to unstable applications or unexpected side effects, even if the original product had been designed with the best practices at that time.
After a certain amount of time, changes become harder to implement. % FIXME language
Sometimes the vendor of frameworks, libraries and third-party software used in a project stops supporting it and the only solution is substituting the components that depend on it. 
The product becomes unmaintainable in many aspects and reenginering is required. 

Software reengineering comprises \textbf{6 activitie}s: \textbf{inventory analysis}, \textbf{document reestructure}, \textbf{reverse engineering}, \textbf{code restructuring}, \textbf{data restructuring} and \textbf{forward engineering}.
Simple put, it is all about understanding the current logic of a program and modifying the three typical components: data, logic and view.
\textbf{The project of this thesis is not modifying the current program but developing a new one with new technology, focusing on reverse engineering the original software}.

Doing an inventory of the software that belongs to the system is part of the definition of the project scope.
Making a list of programs with the current status, dependencies and physical location helps on getting an overview of the project.
Usually software has documentation that needs to be added to the inventory and, sometimes, restructured.
After this, the reverse engineering process can start.

\paragraph{Reverse Engineering}
\textbf{Three dimensions} drive tasks in the reverse engineering activity: \textbf{abstraction level}, \textbf{completeness} and \textbf{directionality}.
To understand what the software does, abstraction should be as high as possible.
This information can have many details or not (completeness) and can be created in one way (extracting it from the source code) or two-way (feed information to the reengineering tool).
This project does not use any computer-assisted reengineering tool.

\subparagraph{Understand current program}
A program normally has \textbf{input data}, \textbf{logic} that processes it and a \textbf{view}:
The first activity is understanding the processing.
Code can be examined at several \textbf{levels of abstraction}: \textbf{system}, \textbf{program}, \textbf{component}, \textbf{pattern}, \textbf{statement}.
Some \textbf{key aspects} include the \textbf{interaction}, \textbf{interoperability} or the \textbf{context}.
Data structures, classes, schemas in databases, etc have to be identified and possibly redesigned in the next steps.
Finally, the \ac{UI} has to be analysed as well. 
There are issues related to presentation of data, interaction, usability, etc that have to be identified.
This project focuses on system and program levels of abstraction and on interoperability.
The software does not have any views or databases to be reconsidered.
However, it does have some formats server-side (in the backend) that are candidates to be part of this process (e.g. definition of settings with \ac{XML} might have to become \ac{JSON} or have a different structure).

\paragraph{Restructure code and data}
Sometimes the solution is restructuring the current program.
The project of this thesis skips this part because a new application is being developed using the output of the first task: understand the current program
Restructuring the code might not be enough in most cases. 
It can fix immediate issues but it is not a long-term solution.
However, when doing so, \ac{TDD} is useful because tests breaks immediately and often, specially if they are running in the background or periodically.
To improve the software design both data and architecture should be restructured.
\ac{TDD} can provide robustness to this process: if there are tests available, the new design can be proven to work, at least, for the example cases. 
If not, the solution can be built incrementally with tests that define the requirements of the system (extracted in the first activity, thus using them as an specification.

\paragraph{Forward engineering}
There is a number of options when it comes to apply changes in the current program. 
For example, apply patches, redesign parts of the software or completely redesign it.
Depending on the circumstances, completely redesigning a software might be costly but, in the long-term, it might be a cost-effective solution.
The project of this thesis redesigns the software to incorporate new practices, new technology and, in the future (out of the scope of the thesis), additional requirements.
The outcome of this activity is a new software that replaces the current implementation.


\section{Web Technology}
% quotations: something about emacs life style, or berners lee
The \ac{WWW} plays a central role in many people's lives.
The first drafts date back to the 1980s, the first web browser prototype by Tim Berners-Lee was built in December 1990 and the Mosaic browser was completed in 1993.
After that, Netscape, Mozilla Firefox, Microsoft Internet Explorer, Google Chrome, Safari, Opera, Konqueror and many other browsers appeared, along with some nowadays popular websites (e.g. Amazon, 1995).

\subsection{From Plain Documents to Applications}
\paragraph{The Evolution of the Web}
Berners-Lee said \cite{BernersLee:1990}:
\begin{quote} 
HyperText is a way to link and access information of various kinds as a web of nodes in which the user can browse at will. 
Potentially, HyperText provides a single user-interface to many large classes of stored information such as reports, notes, data-bases, computer documentation and on-line systems help.
\end{quote} 

When the \ac{WWW} was started in 1990 it was intended to be a document viewing platform.
It has evolved in at least \textbf{three phases} \cite{Anttonen:2011} \cite{Taivalsaari:2008}:

\begin{enumerate}
\item \textbf{Plain text} files, forms, possibly static images and links.
\item \textbf{Programming capabilities} in the browser, plug-ins and animated graphics. 
Web pages became interactive and client-server communication increased in complexity.
\flash (at that time Macromedia Flash), ShockWave, Java, QuickTime and a few others allowed the inclusion of multimedia contents or even programs (e.g. Java Applets).
Server-side and client-side technology was combined to design new contents, interaction and features.
\item \textbf{Applications in the browser}.
Web pages are not just documents: they have complex user interaction, they do not require full page refresh and typical use cases have evolved. 
A big part of applications is executed client-side, taking advantage of the computing capabilities of modern hardware and decreasing the load in servers. 
In some cases, the application also needs huge server-side resources and uses strategies like cloud computing (e.g. Amazon EC2) to scale up.
\end{enumerate}

Today the web is not only about viewing documents but about world-wide sharing, collaboration and interaction, possibly in real time. 
Web browsers have become a widely-used platform for software applications. 
Video editors, spreadsheets, calendars or 3D games used to run exclusively on desktop computers. 
Today, they run in a variety of devices and some of them even do it in a browser. 
There are 3D engines ported to Web, real time collaboration, full HD video support without plug-ins and many more features built-in a system, the browser, that is not an ideal execution environment for desktop applications.

\paragraph{\ac{RIA}}
\acp{RIA} are neither web services or web pages. 
They are software systems based on technologies and standards of the \ac{W3C} that provide web specific resources such as content and services through a web browser \cite{Kappel:2006}.
They are typically designed as single page websites.

% FIXME language
They all have some common characteristics. 
Amongst others:
\begin{itemize}
    \item The product
    \begin{itemize}
        \item \textbf{Content} is the core.
        \item \textbf{Hyper-text}: non-linearity is a main distinction to traditional software systems. There are many ways of landing on a certain page. This can lead to disorientation or cognitive overload for users.
        \item \textbf{Presentation} aesthetics, usability and interaction are closer to a desktop application than to a web page
    \end{itemize}
    \item Use
    \begin{itemize}
        \item \textbf{Globality}: Spontaneity and multiculturality of users. Web applications are publicly available.
        \item Quality suffers from \textbf{unknown network} characteristics
        \item \textbf{Multiplatform delivery} involves having different devices, browsers and degrees of functionality and performance
        \item \textbf{Intense network usage}. Remote calls are minimised. The use of software patterns like remote fa\c{c}ade, remote proxy, \ac{DTO} or \ac{RPC} is frequent
    \end{itemize}
    \item Evolution
    \begin{itemize}
         \item Continuous \textbf{change}
         \item \textbf{Competitive pressure}
         \item \textbf{Fast pace development}
    \end{itemize}   
\end{itemize}

The project of this thesis is, in a sense, both a \ac{RIA} and a \acp{RIA} generator. 
It works in the browser with web technology and has an intense use of the network to fetch the model or other components to build the screens.
The rendered application works in a browser, has an intense use of the network as well and is content-centred.
Despite the fact that it works in the specific context of a robot, globality is still an issue.
A typical scenario would be a fleet of robots in a conference: users from many different cultural backgrounds can use the application at any time.


\subsection{Isolation of Applications in the Browser}    
Many of the websites in the aforementioned third phase contain substantial amounts of client-side code. 
At the end of the day, a \ac{RIA} is a distributed application. 
In the past, the client-side part was thin and simple, whereas today's application have complex logic delegated to client nodes with local storage, hardware-accelerated components, web sockets and other advanced capabilities.
This project uses some of them to, for instance, communicate a browser with the robot or to decrease the system load using hardware-accelerated \ac{CSS} properties.

In spite of the fact that \acp{RIA} have grown in complexity and now demand more resources, browsers architectures still do not provide sufficient isolation between concurrently executing programs.
A similar problem occurred in early operating systems (e.g. MS-DOS), before processes appeared \cite{Reis:2009}. 

Most of browsers have a monolithic architecture with poor isolation between web application instances. Chromium, however, implemented an architecture based on \ac{OS} processes. 
Another way of isolating web applications is running them in a plug-in container. 

The project of this thesis does the first phase with QtWebKit, the port of WebKit on top of Qt, to display the application in a Qt window and communicate with the software in the robot. 
The second part uses Chrome to meet the technology requirements.
QtWebKit components comprise the WebCore and the javascript engine SquirrelFish Extreme, which compiles Javascript into native machine code. 
It is compatible with \flash player but because it will not receive more updates in the future, it will not be used any more.
The \flash plug-in is a black box isolated from other elements in the \ac{DOM}.
Isolation is not an issue in this project because there is only one application running at a time and, even in a normal use of a browser, with many websites running at once, isolation is not a blocking issue.


\section{Test-Driven Development}
\label{sec:testdrivendev}
The software written for this thesis is executed in a complex environment.
Testing it properly and ensuring the quality is crucial to avoid unexpected behaviour in one of the most visible parts of this system, the screen.
Crashes and \ac{UI} flaws can lead to triggering wrong external calls to other components of the system (e.g. displaying a Qt window different than expected, not making the call, sending it with bad parameters...), blocking access to features or seriously affecting the \ac{UX}.

Reengineering this system has, at least, two sources of potential errors: 
undocumented features in the original application and integration of the new software in the current system.

% maybe put all this in a table?
\ac{TDD} is based on having numerous and very short iterations with these steps:
\begin{enumerate}
    \item Add an initial (failing) test
    \item Run all tests and see if the new one fails
    \item Write the minimum amount of code to pass the test
    \item Run the automated tests and see them succeed
    \item Refactor the code. Conform to standards and best practices
\end{enumerate}

As opposed to classic development methodologies (e.g. waterfall, where tests are done in the last place), with \ac{TDD} tests go first. 
Specification and documentation of the system are not artifacts created before the implementation but the tests themselves.

Testing earlier and heavily has benefits over the classical approach:
\begin{description}
\item[Efficiency] Defects (and their causes) are identified earlier.
\item[Robustness] The system is more reliable and stable. 
The \ac{QA} process is stricter and easier to maintain.
\item[Maintainability] Small fixes can introduce new errors in the system. 
Testing continuously and producing the minimum amount of code to implement a feature reduces this.
\item[Extensibility] \ac{XP} embraces change and makes it is easier to adapt to changes in the requirements. 
During the development, the original software keeps adding features and bug fixes.
\end{description}

This methodology is specially useful in the implementation of this project for two reasons: 
the Google Angular framework is extremely friendly with unit testing and \ac{E2E} tests thanks to Karma Runner, and the company has the infrastructure to incorporate the project in a continuous integration system with Jenkins.

\section{XML as an intermediate representation}
FIXME
lorem ipsum
